{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3aa6359-fae6-42a1-bd71-e9d1f0aa7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader\n",
    ")\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io\n",
    "\n",
    "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c3fb4-c606-4939-858d-68601787c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cultivar</th>\n",
       "      <th>cultivar_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000005362.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000099707.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000135300.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000136796.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000292439.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23634</th>\n",
       "      <td>999578153.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23635</th>\n",
       "      <td>999692877.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23636</th>\n",
       "      <td>999756998.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23637</th>\n",
       "      <td>999892248.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23638</th>\n",
       "      <td>999945922.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23639 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename   cultivar  cultivar_index\n",
       "0      1000005362.png  PI_152923               0\n",
       "1      1000099707.png  PI_152923               0\n",
       "2      1000135300.png  PI_152923               0\n",
       "3      1000136796.png  PI_152923               0\n",
       "4      1000292439.png  PI_152923               0\n",
       "...               ...        ...             ...\n",
       "23634   999578153.png  PI_152923               0\n",
       "23635   999692877.png  PI_152923               0\n",
       "23636   999756998.png  PI_152923               0\n",
       "23637   999892248.png  PI_152923               0\n",
       "23638   999945922.png  PI_152923               0\n",
       "\n",
       "[23639 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = os.getcwd()\n",
    "datadir = basedir + '/data'\n",
    "# reading the csv file with annotated image file names\n",
    "test_cultivar= pd.read_csv(datadir + '/sample_submission.csv')\n",
    "test_cultivar.dropna(inplace=True)\n",
    "\n",
    "# creating list of unique cultivars\n",
    "labels=list(np.unique(test_cultivar['cultivar']))\n",
    "\n",
    "# turning cultivar labels into strings\n",
    "test_cultivar['cultivar']=test_cultivar['cultivar'].astype(str)\n",
    "test_cultivar[\"cultivar_index\"] = test_cultivar[\"cultivar\"].map(lambda item: labels.index(item))\n",
    "\n",
    "test_cultivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f1d6162b-c995-420b-ba10-9d1dd664d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file /home/maymap/results/checkpoint-300/preprocessor_config.json\n",
      "Feature extractor ConvNextFeatureExtractor {\n",
      "  \"crop_pct\": 0.875,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = basedir + '/results/checkpoint-300'\n",
    "feature_extractor = ConvNextFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# building feature extractor to grab labels\n",
    "class FeatureExtractor(object):\n",
    "    def __call__(self, image, target):\n",
    "        sample = feature_extractor(image, return_tensors='pt')\n",
    "        sample[\"labels\"] = target\n",
    "        return sample\n",
    "\n",
    "class CultivarDataset(Dataset):\n",
    "    def __init__(self, df_img, df_label, transform):\n",
    "        self.labels = df_label\n",
    "        self.image_path = df_img\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = datadir + '/train_images/' + self.image_path[idx]\n",
    "        image = io.imread(image_path)\n",
    "        \n",
    "        y_label = torch.tensor(int(self.labels.iloc[idx]))\n",
    "       # y_label = int(self.labels.iloc[idx])\n",
    "        \n",
    "        data = self.transform(image,y_label)\n",
    "        data['pixel_values'] = torch.squeeze(data['pixel_values'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6c592dc3-6a07-450a-ba18-064803c20f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CultivarDataset(\n",
    "    df_img = test_cultivar['filename'],\n",
    "    df_label = test_cultivar['cultivar_index'],\n",
    "    transform = FeatureExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55513df3-c605-47cf-902a-313e81f434e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1.0733, -0.9363, -0.9705,  ..., -1.1589, -1.1418, -1.1589],\n",
       "         [-0.9705, -0.9363, -0.9534,  ..., -1.1589, -1.1760, -1.1760],\n",
       "         [-1.0390, -1.0390, -0.8335,  ..., -1.0562, -1.0904, -1.1075],\n",
       "         ...,\n",
       "         [-0.9705, -1.0219, -1.1247,  ..., -0.6452, -0.6794, -0.8678],\n",
       "         [-1.1418, -1.1760, -1.2617,  ..., -0.7308, -0.9877, -1.0562],\n",
       "         [-1.3130, -1.3130, -1.3815,  ...,  0.2796,  0.1426,  0.1768]],\n",
       "\n",
       "        [[-0.9503, -0.8277, -0.8277,  ..., -0.9853, -0.9853, -1.0028],\n",
       "         [-0.8627, -0.7927, -0.8277,  ..., -1.0378, -1.0378, -1.0553],\n",
       "         [-0.9328, -0.9503, -0.7402,  ..., -0.8978, -0.9328, -0.9678],\n",
       "         ...,\n",
       "         [-0.7752, -0.8102, -0.9153,  ..., -0.3901, -0.5126, -0.7052],\n",
       "         [-1.0028, -1.0553, -1.1253,  ..., -0.5651, -0.7927, -0.8277],\n",
       "         [-1.1779, -1.2304, -1.2654,  ...,  0.4678,  0.3627,  0.3803]],\n",
       "\n",
       "        [[-1.0027, -0.9156, -0.8807,  ..., -1.0027, -0.9678, -1.0027],\n",
       "         [-0.9156, -0.8110, -0.8284,  ..., -1.0201, -1.0376, -1.0376],\n",
       "         [-0.9330, -0.9156, -0.7761,  ..., -0.9504, -0.9853, -0.9853],\n",
       "         ...,\n",
       "         [-0.7761, -0.8458, -0.9330,  ..., -0.6018, -0.6890, -0.8458],\n",
       "         [-0.9853, -1.0376, -1.0898,  ..., -0.7238, -0.8633, -0.8633],\n",
       "         [-1.1247, -1.1596, -1.1596,  ..., -0.1138, -0.2010, -0.1487]]]), 'labels': tensor(0)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6360031-180a-40f5-816f-625398a1bd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_ds.__getitem__(9)['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1966431-c40b-4b4f-982a-17b879cb18b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/maymap/results/checkpoint-300/config.json\n",
      "Model config ConvNextConfig {\n",
      "  \"_name_or_path\": \"facebook/convnext-tiny-224\",\n",
      "  \"architectures\": [\n",
      "    \"ConvNextForImageClassification\"\n",
      "  ],\n",
      "  \"depths\": [\n",
      "    3,\n",
      "    3,\n",
      "    9,\n",
      "    3\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_sizes\": [\n",
      "    96,\n",
      "    192,\n",
      "    384,\n",
      "    768\n",
      "  ],\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PI_144134\",\n",
      "    \"1\": \"PI_145619\",\n",
      "    \"2\": \"PI_145626\",\n",
      "    \"3\": \"PI_145633\",\n",
      "    \"4\": \"PI_146890\",\n",
      "    \"5\": \"PI_152591\",\n",
      "    \"6\": \"PI_152651\",\n",
      "    \"7\": \"PI_152694\",\n",
      "    \"8\": \"PI_152727\",\n",
      "    \"9\": \"PI_152728\",\n",
      "    \"10\": \"PI_152730\",\n",
      "    \"11\": \"PI_152733\",\n",
      "    \"12\": \"PI_152751\",\n",
      "    \"13\": \"PI_152771\",\n",
      "    \"14\": \"PI_152816\",\n",
      "    \"15\": \"PI_152828\",\n",
      "    \"16\": \"PI_152860\",\n",
      "    \"17\": \"PI_152862\",\n",
      "    \"18\": \"PI_152923\",\n",
      "    \"19\": \"PI_152961\",\n",
      "    \"20\": \"PI_152965\",\n",
      "    \"21\": \"PI_152966\",\n",
      "    \"22\": \"PI_152967\",\n",
      "    \"23\": \"PI_152971\",\n",
      "    \"24\": \"PI_153877\",\n",
      "    \"25\": \"PI_154750\",\n",
      "    \"26\": \"PI_154844\",\n",
      "    \"27\": \"PI_154846\",\n",
      "    \"28\": \"PI_154944\",\n",
      "    \"29\": \"PI_154987\",\n",
      "    \"30\": \"PI_154988\",\n",
      "    \"31\": \"PI_155516\",\n",
      "    \"32\": \"PI_155760\",\n",
      "    \"33\": \"PI_155885\",\n",
      "    \"34\": \"PI_156178\",\n",
      "    \"35\": \"PI_156217\",\n",
      "    \"36\": \"PI_156268\",\n",
      "    \"37\": \"PI_156326\",\n",
      "    \"38\": \"PI_156330\",\n",
      "    \"39\": \"PI_156393\",\n",
      "    \"40\": \"PI_156463\",\n",
      "    \"41\": \"PI_156487\",\n",
      "    \"42\": \"PI_156871\",\n",
      "    \"43\": \"PI_156890\",\n",
      "    \"44\": \"PI_157030\",\n",
      "    \"45\": \"PI_157035\",\n",
      "    \"46\": \"PI_157804\",\n",
      "    \"47\": \"PI_167093\",\n",
      "    \"48\": \"PI_170787\",\n",
      "    \"49\": \"PI_175919\",\n",
      "    \"50\": \"PI_176766\",\n",
      "    \"51\": \"PI_179749\",\n",
      "    \"52\": \"PI_180348\",\n",
      "    \"53\": \"PI_181080\",\n",
      "    \"54\": \"PI_181083\",\n",
      "    \"55\": \"PI_195754\",\n",
      "    \"56\": \"PI_196049\",\n",
      "    \"57\": \"PI_196583\",\n",
      "    \"58\": \"PI_196586\",\n",
      "    \"59\": \"PI_196598\",\n",
      "    \"60\": \"PI_197542\",\n",
      "    \"61\": \"PI_19770\",\n",
      "    \"62\": \"PI_213900\",\n",
      "    \"63\": \"PI_217691\",\n",
      "    \"64\": \"PI_218112\",\n",
      "    \"65\": \"PI_221548\",\n",
      "    \"66\": \"PI_221651\",\n",
      "    \"67\": \"PI_22913\",\n",
      "    \"68\": \"PI_229841\",\n",
      "    \"69\": \"PI_251672\",\n",
      "    \"70\": \"PI_253986\",\n",
      "    \"71\": \"PI_255239\",\n",
      "    \"72\": \"PI_255744\",\n",
      "    \"73\": \"PI_257599\",\n",
      "    \"74\": \"PI_257600\",\n",
      "    \"75\": \"PI_266927\",\n",
      "    \"76\": \"PI_267573\",\n",
      "    \"77\": \"PI_273465\",\n",
      "    \"78\": \"PI_273969\",\n",
      "    \"79\": \"PI_276837\",\n",
      "    \"80\": \"PI_297130\",\n",
      "    \"81\": \"PI_297155\",\n",
      "    \"82\": \"PI_297171\",\n",
      "    \"83\": \"PI_302252\",\n",
      "    \"84\": \"PI_303658\",\n",
      "    \"85\": \"PI_329256\",\n",
      "    \"86\": \"PI_329286\",\n",
      "    \"87\": \"PI_329299\",\n",
      "    \"88\": \"PI_329300\",\n",
      "    \"89\": \"PI_329301\",\n",
      "    \"90\": \"PI_329310\",\n",
      "    \"91\": \"PI_329319\",\n",
      "    \"92\": \"PI_329326\",\n",
      "    \"93\": \"PI_329333\",\n",
      "    \"94\": \"PI_329338\",\n",
      "    \"95\": \"PI_329351\",\n",
      "    \"96\": \"PI_35038\",\n",
      "    \"97\": \"PI_52606\",\n",
      "    \"98\": \"PI_63715\",\n",
      "    \"99\": \"PI_92270\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"PI_144134\": \"0\",\n",
      "    \"PI_145619\": \"1\",\n",
      "    \"PI_145626\": \"2\",\n",
      "    \"PI_145633\": \"3\",\n",
      "    \"PI_146890\": \"4\",\n",
      "    \"PI_152591\": \"5\",\n",
      "    \"PI_152651\": \"6\",\n",
      "    \"PI_152694\": \"7\",\n",
      "    \"PI_152727\": \"8\",\n",
      "    \"PI_152728\": \"9\",\n",
      "    \"PI_152730\": \"10\",\n",
      "    \"PI_152733\": \"11\",\n",
      "    \"PI_152751\": \"12\",\n",
      "    \"PI_152771\": \"13\",\n",
      "    \"PI_152816\": \"14\",\n",
      "    \"PI_152828\": \"15\",\n",
      "    \"PI_152860\": \"16\",\n",
      "    \"PI_152862\": \"17\",\n",
      "    \"PI_152923\": \"18\",\n",
      "    \"PI_152961\": \"19\",\n",
      "    \"PI_152965\": \"20\",\n",
      "    \"PI_152966\": \"21\",\n",
      "    \"PI_152967\": \"22\",\n",
      "    \"PI_152971\": \"23\",\n",
      "    \"PI_153877\": \"24\",\n",
      "    \"PI_154750\": \"25\",\n",
      "    \"PI_154844\": \"26\",\n",
      "    \"PI_154846\": \"27\",\n",
      "    \"PI_154944\": \"28\",\n",
      "    \"PI_154987\": \"29\",\n",
      "    \"PI_154988\": \"30\",\n",
      "    \"PI_155516\": \"31\",\n",
      "    \"PI_155760\": \"32\",\n",
      "    \"PI_155885\": \"33\",\n",
      "    \"PI_156178\": \"34\",\n",
      "    \"PI_156217\": \"35\",\n",
      "    \"PI_156268\": \"36\",\n",
      "    \"PI_156326\": \"37\",\n",
      "    \"PI_156330\": \"38\",\n",
      "    \"PI_156393\": \"39\",\n",
      "    \"PI_156463\": \"40\",\n",
      "    \"PI_156487\": \"41\",\n",
      "    \"PI_156871\": \"42\",\n",
      "    \"PI_156890\": \"43\",\n",
      "    \"PI_157030\": \"44\",\n",
      "    \"PI_157035\": \"45\",\n",
      "    \"PI_157804\": \"46\",\n",
      "    \"PI_167093\": \"47\",\n",
      "    \"PI_170787\": \"48\",\n",
      "    \"PI_175919\": \"49\",\n",
      "    \"PI_176766\": \"50\",\n",
      "    \"PI_179749\": \"51\",\n",
      "    \"PI_180348\": \"52\",\n",
      "    \"PI_181080\": \"53\",\n",
      "    \"PI_181083\": \"54\",\n",
      "    \"PI_195754\": \"55\",\n",
      "    \"PI_196049\": \"56\",\n",
      "    \"PI_196583\": \"57\",\n",
      "    \"PI_196586\": \"58\",\n",
      "    \"PI_196598\": \"59\",\n",
      "    \"PI_197542\": \"60\",\n",
      "    \"PI_19770\": \"61\",\n",
      "    \"PI_213900\": \"62\",\n",
      "    \"PI_217691\": \"63\",\n",
      "    \"PI_218112\": \"64\",\n",
      "    \"PI_221548\": \"65\",\n",
      "    \"PI_221651\": \"66\",\n",
      "    \"PI_22913\": \"67\",\n",
      "    \"PI_229841\": \"68\",\n",
      "    \"PI_251672\": \"69\",\n",
      "    \"PI_253986\": \"70\",\n",
      "    \"PI_255239\": \"71\",\n",
      "    \"PI_255744\": \"72\",\n",
      "    \"PI_257599\": \"73\",\n",
      "    \"PI_257600\": \"74\",\n",
      "    \"PI_266927\": \"75\",\n",
      "    \"PI_267573\": \"76\",\n",
      "    \"PI_273465\": \"77\",\n",
      "    \"PI_273969\": \"78\",\n",
      "    \"PI_276837\": \"79\",\n",
      "    \"PI_297130\": \"80\",\n",
      "    \"PI_297155\": \"81\",\n",
      "    \"PI_297171\": \"82\",\n",
      "    \"PI_302252\": \"83\",\n",
      "    \"PI_303658\": \"84\",\n",
      "    \"PI_329256\": \"85\",\n",
      "    \"PI_329286\": \"86\",\n",
      "    \"PI_329299\": \"87\",\n",
      "    \"PI_329300\": \"88\",\n",
      "    \"PI_329301\": \"89\",\n",
      "    \"PI_329310\": \"90\",\n",
      "    \"PI_329319\": \"91\",\n",
      "    \"PI_329326\": \"92\",\n",
      "    \"PI_329333\": \"93\",\n",
      "    \"PI_329338\": \"94\",\n",
      "    \"PI_329351\": \"95\",\n",
      "    \"PI_35038\": \"96\",\n",
      "    \"PI_52606\": \"97\",\n",
      "    \"PI_63715\": \"98\",\n",
      "    \"PI_92270\": \"99\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 1e-06,\n",
      "  \"model_type\": \"convnext\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_stages\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\"\n",
      "}\n",
      "\n",
      "loading weights file /home/maymap/results/checkpoint-300/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ConvNextForImageClassification.\n",
      "\n",
      "All the weights of ConvNextForImageClassification were initialized from the model checkpoint at /home/maymap/results/checkpoint-300.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ConvNextForImageClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 23639\n",
      "  Batch size = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1170' max='985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [985/985 2:26:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "model = ConvNextForImageClassification.from_pretrained(model_name_or_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model\n",
    ")\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "pred = trainer.predict(test_ds).predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90d4cfff-8412-42bb-a816-8c84ecfba694",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir + '/mod1predictions','w') as f:\n",
    "        for row in labels:\n",
    "            f.write(str(row))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1be05554-91dc-4ab7-9d19-ccbf48d4e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# softmax each row so each row sums to 1\n",
    "prob = softmax(pred, axis = 1)\n",
    "culti_index = np.argmax(prob,axis =1)\n",
    "\n",
    "with open(basedir + '/mod1predictions','w') as f:\n",
    "        for row in prob:\n",
    "            f.write(str(row))\n",
    "            f.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0f7ea756-f12d-4ff4-8fe6-a418816c1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_cultivar['cultivar_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9b6f13a9-a7d0-4f6b-9207-67bec579b0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 93, 61, ..., 22, 18, 67])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f58525ce-069f-4b0a-8a04-2bda504d06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv file with annotated image file names\n",
    "train_cultivar= pd.read_csv(datadir + '/train_cultivar_mapping.csv')\n",
    "train_cultivar.dropna(inplace=True)\n",
    "\n",
    "# turning cultivar labels into strings\n",
    "train_cultivar['cultivar']=train_cultivar['cultivar'].astype(str)\n",
    "\n",
    "# creating list of unique cultivars\n",
    "labels=list(np.unique(train_cultivar['cultivar']))\n",
    "\n",
    "# encoding cultivar_index column\n",
    "train_cultivar[\"cultivar_index\"] = train_cultivar[\"cultivar\"].map(lambda item:\n",
    "            labels.index(item))\n",
    "\n",
    "# building label and id dicts\n",
    "label2id, id2label = dict(), dict()\n",
    "for i,label in enumerate(labels):\n",
    "    label2id[label]=str(i)\n",
    "    id2label[str(i)]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "10c5c822-7dec-4545-88bf-8a1258c085a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for val in y_pred:\n",
    "    key_val = str(val)\n",
    "    labels.append(id2label[key_val])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6d95e456-a9fb-4596-852e-806d2b218c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['filename'] = test_cultivar['filename']\n",
    "submission['cultivar'] = labels\n",
    "submission.drop([0])\n",
    "submission.to_csv(basedir+'/mod1submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a6362e8b-38da-4ef0-874c-329514834fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4439\n",
      "  Batch size = 24\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_cultivar['image'],train_cultivar[\"cultivar_index\"], test_size = 0.2)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# valid dataset\n",
    "valid_ds = CultivarDataset(\n",
    "    df_img = X_test,\n",
    "    df_label = y_test,\n",
    "    transform=FeatureExtractor(),\n",
    ")\n",
    "valid_pred = trainer.predict(valid_ds).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca4fd9-dbb6-4058-8b77-6793cac944e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
