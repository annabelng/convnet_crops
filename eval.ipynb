{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3aa6359-fae6-42a1-bd71-e9d1f0aa7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader\n",
    ")\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io\n",
    "\n",
    "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516c3fb4-c606-4939-858d-68601787c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cultivar</th>\n",
       "      <th>cultivar_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000005362.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000099707.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000135300.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000136796.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000292439.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23634</th>\n",
       "      <td>999578153.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23635</th>\n",
       "      <td>999692877.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23636</th>\n",
       "      <td>999756998.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23637</th>\n",
       "      <td>999892248.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23638</th>\n",
       "      <td>999945922.png</td>\n",
       "      <td>PI_152923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23639 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename   cultivar  cultivar_index\n",
       "0      1000005362.png  PI_152923               0\n",
       "1      1000099707.png  PI_152923               0\n",
       "2      1000135300.png  PI_152923               0\n",
       "3      1000136796.png  PI_152923               0\n",
       "4      1000292439.png  PI_152923               0\n",
       "...               ...        ...             ...\n",
       "23634   999578153.png  PI_152923               0\n",
       "23635   999692877.png  PI_152923               0\n",
       "23636   999756998.png  PI_152923               0\n",
       "23637   999892248.png  PI_152923               0\n",
       "23638   999945922.png  PI_152923               0\n",
       "\n",
       "[23639 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basedir = os.getcwd()\n",
    "datadir = basedir + '/data'\n",
    "# reading the csv file with annotated image file names\n",
    "test_cultivar= pd.read_csv(datadir + '/sample_submission.csv')\n",
    "test_cultivar.dropna(inplace=True)\n",
    "\n",
    "# creating list of unique cultivars\n",
    "labels=list(np.unique(test_cultivar['cultivar']))\n",
    "\n",
    "# turning cultivar labels into strings\n",
    "test_cultivar['cultivar']=test_cultivar['cultivar'].astype(str)\n",
    "test_cultivar[\"cultivar_index\"] = test_cultivar[\"cultivar\"].map(lambda item: labels.index(item))\n",
    "\n",
    "test_cultivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f1d6162b-c995-420b-ba10-9d1dd664d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file /home/maymap/results/checkpoint-300/preprocessor_config.json\n",
      "Feature extractor ConvNextFeatureExtractor {\n",
      "  \"crop_pct\": 0.875,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = basedir + '/results/checkpoint-300'\n",
    "feature_extractor = ConvNextFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# building feature extractor to grab labels\n",
    "class FeatureExtractor(object):\n",
    "    def __call__(self, image, target):\n",
    "        sample = feature_extractor(image, return_tensors='pt')\n",
    "        sample[\"labels\"] = target\n",
    "        return sample\n",
    "\n",
    "class CultivarDataset(Dataset):\n",
    "    def __init__(self, df_img, df_label, transform):\n",
    "        self.labels = df_label\n",
    "        self.image_path = df_img\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = datadir + '/test/' + self.image_path[idx]\n",
    "        image = io.imread(image_path)\n",
    "        \n",
    "        y_label = torch.tensor(int(self.labels.iloc[idx]))\n",
    "       # y_label = int(self.labels.iloc[idx])\n",
    "        \n",
    "        data = self.transform(image,y_label)\n",
    "        data['pixel_values'] = torch.squeeze(data['pixel_values'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6c592dc3-6a07-450a-ba18-064803c20f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CultivarDataset(\n",
    "    df_img = test_cultivar['filename'],\n",
    "    df_label = test_cultivar['cultivar_index'],\n",
    "    transform = FeatureExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55513df3-c605-47cf-902a-313e81f434e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1.0733, -0.9363, -0.9705,  ..., -1.1589, -1.1418, -1.1589],\n",
       "         [-0.9705, -0.9363, -0.9534,  ..., -1.1589, -1.1760, -1.1760],\n",
       "         [-1.0390, -1.0390, -0.8335,  ..., -1.0562, -1.0904, -1.1075],\n",
       "         ...,\n",
       "         [-0.9705, -1.0219, -1.1247,  ..., -0.6452, -0.6794, -0.8678],\n",
       "         [-1.1418, -1.1760, -1.2617,  ..., -0.7308, -0.9877, -1.0562],\n",
       "         [-1.3130, -1.3130, -1.3815,  ...,  0.2796,  0.1426,  0.1768]],\n",
       "\n",
       "        [[-0.9503, -0.8277, -0.8277,  ..., -0.9853, -0.9853, -1.0028],\n",
       "         [-0.8627, -0.7927, -0.8277,  ..., -1.0378, -1.0378, -1.0553],\n",
       "         [-0.9328, -0.9503, -0.7402,  ..., -0.8978, -0.9328, -0.9678],\n",
       "         ...,\n",
       "         [-0.7752, -0.8102, -0.9153,  ..., -0.3901, -0.5126, -0.7052],\n",
       "         [-1.0028, -1.0553, -1.1253,  ..., -0.5651, -0.7927, -0.8277],\n",
       "         [-1.1779, -1.2304, -1.2654,  ...,  0.4678,  0.3627,  0.3803]],\n",
       "\n",
       "        [[-1.0027, -0.9156, -0.8807,  ..., -1.0027, -0.9678, -1.0027],\n",
       "         [-0.9156, -0.8110, -0.8284,  ..., -1.0201, -1.0376, -1.0376],\n",
       "         [-0.9330, -0.9156, -0.7761,  ..., -0.9504, -0.9853, -0.9853],\n",
       "         ...,\n",
       "         [-0.7761, -0.8458, -0.9330,  ..., -0.6018, -0.6890, -0.8458],\n",
       "         [-0.9853, -1.0376, -1.0898,  ..., -0.7238, -0.8633, -0.8633],\n",
       "         [-1.1247, -1.1596, -1.1596,  ..., -0.1138, -0.2010, -0.1487]]]), 'labels': tensor(0)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6360031-180a-40f5-816f-625398a1bd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_ds.__getitem__(9)['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1966431-c40b-4b4f-982a-17b879cb18b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/maymap/results/checkpoint-300/config.json\n",
      "Model config ConvNextConfig {\n",
      "  \"_name_or_path\": \"facebook/convnext-tiny-224\",\n",
      "  \"architectures\": [\n",
      "    \"ConvNextForImageClassification\"\n",
      "  ],\n",
      "  \"depths\": [\n",
      "    3,\n",
      "    3,\n",
      "    9,\n",
      "    3\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_sizes\": [\n",
      "    96,\n",
      "    192,\n",
      "    384,\n",
      "    768\n",
      "  ],\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PI_144134\",\n",
      "    \"1\": \"PI_145619\",\n",
      "    \"2\": \"PI_145626\",\n",
      "    \"3\": \"PI_145633\",\n",
      "    \"4\": \"PI_146890\",\n",
      "    \"5\": \"PI_152591\",\n",
      "    \"6\": \"PI_152651\",\n",
      "    \"7\": \"PI_152694\",\n",
      "    \"8\": \"PI_152727\",\n",
      "    \"9\": \"PI_152728\",\n",
      "    \"10\": \"PI_152730\",\n",
      "    \"11\": \"PI_152733\",\n",
      "    \"12\": \"PI_152751\",\n",
      "    \"13\": \"PI_152771\",\n",
      "    \"14\": \"PI_152816\",\n",
      "    \"15\": \"PI_152828\",\n",
      "    \"16\": \"PI_152860\",\n",
      "    \"17\": \"PI_152862\",\n",
      "    \"18\": \"PI_152923\",\n",
      "    \"19\": \"PI_152961\",\n",
      "    \"20\": \"PI_152965\",\n",
      "    \"21\": \"PI_152966\",\n",
      "    \"22\": \"PI_152967\",\n",
      "    \"23\": \"PI_152971\",\n",
      "    \"24\": \"PI_153877\",\n",
      "    \"25\": \"PI_154750\",\n",
      "    \"26\": \"PI_154844\",\n",
      "    \"27\": \"PI_154846\",\n",
      "    \"28\": \"PI_154944\",\n",
      "    \"29\": \"PI_154987\",\n",
      "    \"30\": \"PI_154988\",\n",
      "    \"31\": \"PI_155516\",\n",
      "    \"32\": \"PI_155760\",\n",
      "    \"33\": \"PI_155885\",\n",
      "    \"34\": \"PI_156178\",\n",
      "    \"35\": \"PI_156217\",\n",
      "    \"36\": \"PI_156268\",\n",
      "    \"37\": \"PI_156326\",\n",
      "    \"38\": \"PI_156330\",\n",
      "    \"39\": \"PI_156393\",\n",
      "    \"40\": \"PI_156463\",\n",
      "    \"41\": \"PI_156487\",\n",
      "    \"42\": \"PI_156871\",\n",
      "    \"43\": \"PI_156890\",\n",
      "    \"44\": \"PI_157030\",\n",
      "    \"45\": \"PI_157035\",\n",
      "    \"46\": \"PI_157804\",\n",
      "    \"47\": \"PI_167093\",\n",
      "    \"48\": \"PI_170787\",\n",
      "    \"49\": \"PI_175919\",\n",
      "    \"50\": \"PI_176766\",\n",
      "    \"51\": \"PI_179749\",\n",
      "    \"52\": \"PI_180348\",\n",
      "    \"53\": \"PI_181080\",\n",
      "    \"54\": \"PI_181083\",\n",
      "    \"55\": \"PI_195754\",\n",
      "    \"56\": \"PI_196049\",\n",
      "    \"57\": \"PI_196583\",\n",
      "    \"58\": \"PI_196586\",\n",
      "    \"59\": \"PI_196598\",\n",
      "    \"60\": \"PI_197542\",\n",
      "    \"61\": \"PI_19770\",\n",
      "    \"62\": \"PI_213900\",\n",
      "    \"63\": \"PI_217691\",\n",
      "    \"64\": \"PI_218112\",\n",
      "    \"65\": \"PI_221548\",\n",
      "    \"66\": \"PI_221651\",\n",
      "    \"67\": \"PI_22913\",\n",
      "    \"68\": \"PI_229841\",\n",
      "    \"69\": \"PI_251672\",\n",
      "    \"70\": \"PI_253986\",\n",
      "    \"71\": \"PI_255239\",\n",
      "    \"72\": \"PI_255744\",\n",
      "    \"73\": \"PI_257599\",\n",
      "    \"74\": \"PI_257600\",\n",
      "    \"75\": \"PI_266927\",\n",
      "    \"76\": \"PI_267573\",\n",
      "    \"77\": \"PI_273465\",\n",
      "    \"78\": \"PI_273969\",\n",
      "    \"79\": \"PI_276837\",\n",
      "    \"80\": \"PI_297130\",\n",
      "    \"81\": \"PI_297155\",\n",
      "    \"82\": \"PI_297171\",\n",
      "    \"83\": \"PI_302252\",\n",
      "    \"84\": \"PI_303658\",\n",
      "    \"85\": \"PI_329256\",\n",
      "    \"86\": \"PI_329286\",\n",
      "    \"87\": \"PI_329299\",\n",
      "    \"88\": \"PI_329300\",\n",
      "    \"89\": \"PI_329301\",\n",
      "    \"90\": \"PI_329310\",\n",
      "    \"91\": \"PI_329319\",\n",
      "    \"92\": \"PI_329326\",\n",
      "    \"93\": \"PI_329333\",\n",
      "    \"94\": \"PI_329338\",\n",
      "    \"95\": \"PI_329351\",\n",
      "    \"96\": \"PI_35038\",\n",
      "    \"97\": \"PI_52606\",\n",
      "    \"98\": \"PI_63715\",\n",
      "    \"99\": \"PI_92270\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"PI_144134\": \"0\",\n",
      "    \"PI_145619\": \"1\",\n",
      "    \"PI_145626\": \"2\",\n",
      "    \"PI_145633\": \"3\",\n",
      "    \"PI_146890\": \"4\",\n",
      "    \"PI_152591\": \"5\",\n",
      "    \"PI_152651\": \"6\",\n",
      "    \"PI_152694\": \"7\",\n",
      "    \"PI_152727\": \"8\",\n",
      "    \"PI_152728\": \"9\",\n",
      "    \"PI_152730\": \"10\",\n",
      "    \"PI_152733\": \"11\",\n",
      "    \"PI_152751\": \"12\",\n",
      "    \"PI_152771\": \"13\",\n",
      "    \"PI_152816\": \"14\",\n",
      "    \"PI_152828\": \"15\",\n",
      "    \"PI_152860\": \"16\",\n",
      "    \"PI_152862\": \"17\",\n",
      "    \"PI_152923\": \"18\",\n",
      "    \"PI_152961\": \"19\",\n",
      "    \"PI_152965\": \"20\",\n",
      "    \"PI_152966\": \"21\",\n",
      "    \"PI_152967\": \"22\",\n",
      "    \"PI_152971\": \"23\",\n",
      "    \"PI_153877\": \"24\",\n",
      "    \"PI_154750\": \"25\",\n",
      "    \"PI_154844\": \"26\",\n",
      "    \"PI_154846\": \"27\",\n",
      "    \"PI_154944\": \"28\",\n",
      "    \"PI_154987\": \"29\",\n",
      "    \"PI_154988\": \"30\",\n",
      "    \"PI_155516\": \"31\",\n",
      "    \"PI_155760\": \"32\",\n",
      "    \"PI_155885\": \"33\",\n",
      "    \"PI_156178\": \"34\",\n",
      "    \"PI_156217\": \"35\",\n",
      "    \"PI_156268\": \"36\",\n",
      "    \"PI_156326\": \"37\",\n",
      "    \"PI_156330\": \"38\",\n",
      "    \"PI_156393\": \"39\",\n",
      "    \"PI_156463\": \"40\",\n",
      "    \"PI_156487\": \"41\",\n",
      "    \"PI_156871\": \"42\",\n",
      "    \"PI_156890\": \"43\",\n",
      "    \"PI_157030\": \"44\",\n",
      "    \"PI_157035\": \"45\",\n",
      "    \"PI_157804\": \"46\",\n",
      "    \"PI_167093\": \"47\",\n",
      "    \"PI_170787\": \"48\",\n",
      "    \"PI_175919\": \"49\",\n",
      "    \"PI_176766\": \"50\",\n",
      "    \"PI_179749\": \"51\",\n",
      "    \"PI_180348\": \"52\",\n",
      "    \"PI_181080\": \"53\",\n",
      "    \"PI_181083\": \"54\",\n",
      "    \"PI_195754\": \"55\",\n",
      "    \"PI_196049\": \"56\",\n",
      "    \"PI_196583\": \"57\",\n",
      "    \"PI_196586\": \"58\",\n",
      "    \"PI_196598\": \"59\",\n",
      "    \"PI_197542\": \"60\",\n",
      "    \"PI_19770\": \"61\",\n",
      "    \"PI_213900\": \"62\",\n",
      "    \"PI_217691\": \"63\",\n",
      "    \"PI_218112\": \"64\",\n",
      "    \"PI_221548\": \"65\",\n",
      "    \"PI_221651\": \"66\",\n",
      "    \"PI_22913\": \"67\",\n",
      "    \"PI_229841\": \"68\",\n",
      "    \"PI_251672\": \"69\",\n",
      "    \"PI_253986\": \"70\",\n",
      "    \"PI_255239\": \"71\",\n",
      "    \"PI_255744\": \"72\",\n",
      "    \"PI_257599\": \"73\",\n",
      "    \"PI_257600\": \"74\",\n",
      "    \"PI_266927\": \"75\",\n",
      "    \"PI_267573\": \"76\",\n",
      "    \"PI_273465\": \"77\",\n",
      "    \"PI_273969\": \"78\",\n",
      "    \"PI_276837\": \"79\",\n",
      "    \"PI_297130\": \"80\",\n",
      "    \"PI_297155\": \"81\",\n",
      "    \"PI_297171\": \"82\",\n",
      "    \"PI_302252\": \"83\",\n",
      "    \"PI_303658\": \"84\",\n",
      "    \"PI_329256\": \"85\",\n",
      "    \"PI_329286\": \"86\",\n",
      "    \"PI_329299\": \"87\",\n",
      "    \"PI_329300\": \"88\",\n",
      "    \"PI_329301\": \"89\",\n",
      "    \"PI_329310\": \"90\",\n",
      "    \"PI_329319\": \"91\",\n",
      "    \"PI_329326\": \"92\",\n",
      "    \"PI_329333\": \"93\",\n",
      "    \"PI_329338\": \"94\",\n",
      "    \"PI_329351\": \"95\",\n",
      "    \"PI_35038\": \"96\",\n",
      "    \"PI_52606\": \"97\",\n",
      "    \"PI_63715\": \"98\",\n",
      "    \"PI_92270\": \"99\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_scale_init_value\": 1e-06,\n",
      "  \"model_type\": \"convnext\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_stages\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\"\n",
      "}\n",
      "\n",
      "loading weights file /home/maymap/results/checkpoint-300/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ConvNextForImageClassification.\n",
      "\n",
      "All the weights of ConvNextForImageClassification were initialized from the model checkpoint at /home/maymap/results/checkpoint-300.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ConvNextForImageClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 23639\n",
      "  Batch size = 24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='224' max='985' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [224/985 05:55 < 20:13, 0.63 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "model = ConvNextForImageClassification.from_pretrained(model_name_or_path)\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'][0] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./results\",\n",
    "  logging_dir = '/home/runs',\n",
    "  evaluation_strategy='steps',\n",
    "  per_device_train_batch_size=64,\n",
    "  num_train_epochs=4,\n",
    "  save_total_limit = 4, # Only last 4 models are saved. Older ones are deleted.\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "trainer = trainer = Trainer(\n",
    "    model=model\n",
    ")\n",
    "trainer.predict(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4cfff-8412-42bb-a816-8c84ecfba694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
